@article{Anand2011,
abstract = {Page 1. Seminar Report On Google  File  System Submitted by SARITHA.S ... KOCHI 682022 Certificate This is to certify that the seminar report titled ' GOOGLE  FILE  SYSTEM ' submitted by Saritha S, in partial fulfillment of the requirements of the award of MTech Degree in ...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Anand, Rajaraman and Jeffrey, D Ullman},
doi = {10.1017/CBO9781139924801},
eprint = {arXiv:1011.1669v3},
file = {:home/sztal/Dokumenty/Mendeley-Desktop/mining-of-massive-datasets.pdf:pdf},
isbn = {9781107077232},
issn = {01420615},
journal = {2011-01-03. Http://Infolab. Stanford},
pmid = {1107015359},
title = {{Mining of massive datasets}},
url = {http://scholar.google.com/scholar?q=related:o4rvxS-5BtwJ:scholar.google.com/{\&}hl=en{\&}num=20{\&}as{\_}sdt=0,5},
year = {2011}
}
@book{P.Murphy1991,
abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Murphy}, Kevin},
booktitle = {Machine Learning: A Probabilistic Perspective},
doi = {10.1007/SpringerReference_35834},
eprint = {0-387-31073-8},
file = {:home/sztal/Dokumenty/Mendeley-Desktop/(Adaptive Computation and Machine Learning) Kevin P. Murphy-Machine Learning{\_} A Probabilistic Perspective-The MIT Press (2012).pdf:pdf},
isbn = {9780262018029},
issn = {0262018020},
pages = {73--78,216--244},
pmid = {20236947},
title = {{Machine Learning: A Probabilistic Perspective}},
url = {http://link.springer.com/chapter/10.1007/978-94-011-3532-0{\_}2},
year = {1991}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:home/sztal/Dokumenty/Mendeley-Desktop/Ng-LDA.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@article{Kosinski2016,
abstract = {This article aims to introduce the reader to essential tools that can be used to obtain insights and build predictive models using large data sets. Recent user proliferation in the digital environment has led to the emergence of large samples containing a wealth of traces of human behaviors, communication, and social interactions. Such samples offer the opportunity to greatly improve our understanding of individuals, groups, and societies, but their analysis presents unique methodological challenges. In this tutorial, we discuss potential sources of such data and explain how to efficiently store them. Then, we introduce two methods that are often employed to extract patterns and reduce the dimensionality of large data sets: singular value decomposition and latent Dirichlet allocation. Finally, we demonstrate how to use dimensions or clusters extracted from data to build predictive models in a cross-validated way. The text is accompanied by examples of R code and a sample data set, allowing the reader to practice the methods discussed here. A companion website (http://dataminingtutorial.com) provides additional learning resources.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kosinski, Michal and Wang, Yilun and Lakkaraju, Himabindu and Leskovec, Jure},
doi = {10.1037/met0000105},
eprint = {arXiv:1011.1669v3},
file = {:home/sztal/Dokumenty/Mendeley-Desktop/Tutorial Kosinski 2016-57141-003.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {R,big data,computational social science,digital footprints,personality},
number = {4},
pages = {493--506},
pmid = {25246403},
title = {{Mining Big Data to Extract Patterns and Predict Real-Life Outcomes}},
url = {http://psycnet.apa.org/fulltext/2016-57141-003.pdf},
volume = {21},
year = {2016}
}
